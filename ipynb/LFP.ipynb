{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tslearn.utils import to_time_series_dataset\n",
    "from tslearn.metrics import dtw\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from tslearn.utils import to_time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function from NeuroChat - read LFP\n",
    "def load_lfp_Axona(file_name):\n",
    "\n",
    "    file_directory, file_basename = os.path.split(file_name)\n",
    "    file_tag, file_extension = os.path.splitext(file_basename)\n",
    "    file_extension = file_extension[1:]\n",
    "    set_file = os.path.join(file_directory, file_tag + '.set')\n",
    "    if os.path.isfile(file_name):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            while True:\n",
    "                line = f.readline()\n",
    "                try:\n",
    "                    line = line.decode('latin-1')\n",
    "                except BaseException:\n",
    "                    break\n",
    "\n",
    "                if line == '':\n",
    "                    break\n",
    "                if line.startswith('trial_date'):\n",
    "                    # Blank eeg file\n",
    "                    if line.strip() == \"trial_date\":\n",
    "                        total_samples = 0\n",
    "                        return\n",
    "                    date = (\n",
    "                        ' '.join(line.replace(',', ' ').split()[1:]))\n",
    "                if line.startswith('trial_time'):\n",
    "                    time = (line.split()[1])\n",
    "                if line.startswith('experimenter'):\n",
    "                    experimenter = (' '.join(line.split()[1:]))\n",
    "                if line.startswith('comments'):\n",
    "                    comments = (' '.join(line.split()[1:]))\n",
    "                if line.startswith('duration'):\n",
    "                    duration = (float(''.join(line.split()[1:])))\n",
    "                if line.startswith('sw_version'):\n",
    "                    file_version = (line.split()[1])\n",
    "                if line.startswith('num_chans'):\n",
    "                    total_channel = (int(''.join(line.split()[1:])))\n",
    "                if line.startswith('sample_rate'):\n",
    "                    sampling_rate = (\n",
    "                        float(''.join(re.findall(r'\\d+.\\d+|\\d+', line))))\n",
    "                if line.startswith('bytes_per_sample'):\n",
    "                    bytes_per_sample = (\n",
    "                        int(''.join(line.split()[1:])))\n",
    "                if line.startswith(\n",
    "                        'num_' + file_extension[:3].upper() + '_samples'):\n",
    "                    total_samples = (int(''.join(line.split()[1:])))\n",
    "                if line.startswith(\"data_start\"):\n",
    "                    break\n",
    "\n",
    "            num_samples = total_samples\n",
    "            f.seek(0, 0)\n",
    "            header_offset = []\n",
    "            while True:\n",
    "                try:\n",
    "                    buff = f.read(10).decode('UTF-8')\n",
    "                except BaseException:\n",
    "                    break\n",
    "                if buff == 'data_start':\n",
    "                    header_offset = f.tell()\n",
    "                    break\n",
    "                else:\n",
    "                    f.seek(-9, 1)\n",
    "\n",
    "            eeg_ID = re.findall(r'\\d+', file_extension)\n",
    "            file_tag = (1 if not eeg_ID else int(eeg_ID[0]))\n",
    "            max_ADC_count = 2**(8 * bytes_per_sample - 1) - 1\n",
    "            max_byte_value = 2**(8 * bytes_per_sample)\n",
    "\n",
    "            with open(set_file, 'r', encoding='latin-1') as f_set:\n",
    "                lines = f_set.readlines()\n",
    "                channel_lines = dict(\n",
    "                    [tuple(map(int, re.findall(r'\\d+.\\d+|\\d+', line)[0].split()))\n",
    "                        for line in lines if line.startswith('EEG_ch_')]\n",
    "                )\n",
    "                channel_id = channel_lines[file_tag]\n",
    "                channel_id = (channel_id)\n",
    "\n",
    "                gain_lines = dict(\n",
    "                    [tuple(map(int, re.findall(r'\\d+.\\d+|\\d+', line)[0].split()))\n",
    "                        for line in lines if 'gain_ch_' in line]\n",
    "                )\n",
    "                gain = gain_lines[channel_id - 1]\n",
    "\n",
    "                for line in lines:\n",
    "                    if line.startswith('ADC_fullscale_mv'):\n",
    "                        fullscale_mv = (\n",
    "                            int(re.findall(r'\\d+.\\d+|d+', line)[0]))\n",
    "                        break\n",
    "                AD_bit_uvolt = 2 * fullscale_mv / \\\n",
    "                    (gain * np.power(2, 8 * bytes_per_sample))\n",
    "\n",
    "            record_size = bytes_per_sample\n",
    "            sample_le = 256**(np.arange(0, bytes_per_sample, 1))\n",
    "\n",
    "            if not header_offset:\n",
    "                print('Error: data_start marker not found!')\n",
    "            else:\n",
    "                f.seek(header_offset, 0)\n",
    "                byte_buffer = np.fromfile(f, dtype='uint8')\n",
    "                len_bytebuffer = len(byte_buffer)\n",
    "                end_offset = len('\\r\\ndata_end\\r')\n",
    "                lfp_wave = np.zeros([num_samples, ], dtype=np.float64)\n",
    "                for k in np.arange(0, bytes_per_sample, 1):\n",
    "                    byte_offset = k\n",
    "                    sample_value = (\n",
    "                        sample_le[k] * byte_buffer[byte_offset:byte_offset + len_bytebuffer - \n",
    "                                                   end_offset - record_size:record_size])\n",
    "                    if sample_value.size < num_samples:\n",
    "                        sample_value = np.append(sample_value, np.zeros(\n",
    "                            [num_samples - sample_value.size, ]))\n",
    "                    sample_value = sample_value.astype(\n",
    "                        np.float64, casting='unsafe', copy=False)\n",
    "                    np.add(lfp_wave, sample_value, out=lfp_wave)\n",
    "                np.putmask(lfp_wave, lfp_wave > max_ADC_count,\n",
    "                            lfp_wave - max_byte_value)\n",
    "\n",
    "                samples = (lfp_wave * AD_bit_uvolt)\n",
    "                timestamp = (\n",
    "                    np.arange(0, num_samples, 1) / sampling_rate)\n",
    "                return samples\n",
    "\n",
    "    else:\n",
    "        print(\"No lfp file found for file {}\".format(file_name))\n",
    "        \n",
    "        \n",
    "class RecPos:\n",
    "    \"\"\"\n",
    "    This data class contains information about the recording position.\n",
    "    Read .pos file\n",
    "    To dos:\n",
    "        * read different numbers of LEDs\n",
    "        * Adapt to NeuroChat\n",
    "    Attributes\n",
    "    ----------\n",
    "    _file_tag : str\n",
    "        The tag of the pos data.\n",
    "    \"\"\"\n",
    "    def __init__(self, file_name):\n",
    "\n",
    "        self.bytes_per_sample = 20 # Axona daqUSB manual\n",
    "        file_directory, file_basename = os.path.split(file_name)\n",
    "        file_tag, file_extension = os.path.splitext(file_basename)\n",
    "        file_extension = file_extension[1:]\n",
    "        self.pos_file = os.path.join(file_directory, file_tag + '.pos')\n",
    "        if os.path.isfile(self.pos_file):\n",
    "            with open(self.pos_file, 'rb') as f:\n",
    "                while True:\n",
    "                    line = f.readline()\n",
    "                    try:\n",
    "                        line = line.decode('latin-1')\n",
    "                    except BaseException:\n",
    "                        break\n",
    "\n",
    "                    if line == '':\n",
    "                        break\n",
    "                    if line.startswith('trial_date'):\n",
    "                        # Blank eeg file\n",
    "                        if line.strip() == \"trial_date\":\n",
    "                            total_samples = 0\n",
    "                            print('No position data.')\n",
    "                            return\n",
    "                        date = (\n",
    "                            ' '.join(line.replace(',', ' ').split()[1:]))\n",
    "                    if line.startswith('num_colours'):\n",
    "                        colors = (int(line.split()[1]))\n",
    "                    if line.startswith('min_x'):\n",
    "                        self.min_x = (int(line.split()[1]))\n",
    "                    if line.startswith('max_x'):\n",
    "                        self.max_x = (int(line.split()[1]))\n",
    "                    if line.startswith('min_y'):\n",
    "                        self.min_y = (int(line.split()[1]))\n",
    "                    if line.startswith('max_y'):\n",
    "                        self.max_y = (int(line.split()[1]))\n",
    "                    if line.startswith('window_min_x'):\n",
    "                        self.window_min_x = (int(line.split()[1]))\n",
    "                    if line.startswith('window_max_x'):\n",
    "                        self.window_max_x = (int(line.split()[1]))\n",
    "                    if line.startswith('window_min_y'):\n",
    "                        self.window_min_y = (int(line.split()[1]))\n",
    "                    if line.startswith('window_max_y'):\n",
    "                        self.window_max_y = (int(line.split()[1]))\n",
    "                    if line.startswith('bytes_per_timestamp'):\n",
    "                        self.bytes_per_tstamp = (int(line.split()[1]))\n",
    "                    if line.startswith('bytes_per_coord'):\n",
    "                        self.bytes_per_coord = (int(line.split()[1]))\n",
    "                    if line.startswith('pixels_per_metre'):\n",
    "                        self.pixels_per_metre = (int(line.split()[1]))\n",
    "                    if line.startswith('num_pos_samples'):\n",
    "                        self.total_samples = (int(line.split()[1]))\n",
    "                    if line.startswith(\"data_start\"):\n",
    "                        break\n",
    "\n",
    "                f.seek(0, 0)\n",
    "                header_offset = []\n",
    "                while True:\n",
    "                    try:\n",
    "                        buff = f.read(10).decode('UTF-8')\n",
    "                    except BaseException:\n",
    "                        break\n",
    "                    if buff == 'data_start':\n",
    "                        header_offset = f.tell()\n",
    "                        break\n",
    "                    else:\n",
    "                        f.seek(-9, 1)\n",
    "\n",
    "                if not header_offset:\n",
    "                    print('Error: data_start marker not found!')\n",
    "                else:\n",
    "                    f.seek(header_offset, 0)\n",
    "                    byte_buffer = np.fromfile(f, dtype='uint8')\n",
    "                    len_bytebuffer = len(byte_buffer)\n",
    "                    end_offset = len('\\r\\ndata_end\\r')\n",
    "                    num_samples = int(len((byte_buffer)- end_offset)/20)\n",
    "                    big_spotx = np.zeros([self.total_samples,1])\n",
    "                    big_spoty = np.zeros([self.total_samples,1])\n",
    "                    little_spotx = np.zeros([self.total_samples,1])\n",
    "                    little_spoty = np.zeros([self.total_samples,1])\n",
    "                    # pos format: t,x1,y1,x2,y2,numpix1,numpix2 => 20 bytes\n",
    "                    for i, k in enumerate(np.arange(0, self.total_samples*20, 20)): # Extract bytes from 20 bytes words\n",
    "                        byte_offset = k\n",
    "                        big_spotx[i] = int(256 * byte_buffer[k+4] + byte_buffer[k+5])  # 4,5 bytes for big LED x\n",
    "                        big_spoty[i] = int(256 * byte_buffer[k+6] + byte_buffer[k+7])  # 6,7 bytes for big LED x\n",
    "                        little_spotx[i] = int(256 * byte_buffer[k+4] + byte_buffer[k+5])\n",
    "                        little_spoty[i] = int(256 * byte_buffer[k+6] + byte_buffer[k+7])\n",
    "                        \n",
    "                    self.raw_position = {'big_spotx': big_spotx, \n",
    "                                         'big_spoty':big_spoty,\n",
    "                                         'little_spotx':little_spotx, \n",
    "                                         'little_spoty':little_spoty}\n",
    "\n",
    "        else:\n",
    "            print(f\"No pos file found for file {file_name}\")\n",
    "\n",
    "\n",
    "    def get_cam_view(self):\n",
    "        self.cam_view = {'min_x':self.min_x, 'max_x':self.max_x,\n",
    "                    'min_y':self.min_y, 'max_y':self.max_y}\n",
    "        return self.cam_view\n",
    "    \n",
    "    \n",
    "    def get_tmaze_start(self):\n",
    "        x,y = self.get_position()\n",
    "        a = x[100:250] \n",
    "        b = y[100:250]\n",
    "        a = pd.Series([n if n != 1023 else np.nan for n in a])\n",
    "        b = pd.Series([n if n != 1023 else np.nan for n in b])\n",
    "        a.clip(0, 500, inplace=True)\n",
    "        b.clip(0, 500, inplace=True)\n",
    "        a.fillna(method = 'backfill', inplace = True)\n",
    "        b.fillna(method = 'backfill', inplace = True)\n",
    "        if a.mean() < 200 and b.mean() > 300:\n",
    "            start = 'top left'\n",
    "        elif a.mean()  > 400 and b.mean() > 300:\n",
    "            start = 'top right'\n",
    "        elif a.mean()  < 200 and b.mean() < 200:\n",
    "            start = 'down left'\n",
    "        elif a.mean()  >300 and b.mean() < 200:\n",
    "            start = 'down right'\n",
    "        else:\n",
    "            start = 'impossible to find'\n",
    "        return start\n",
    "\n",
    "    def get_window_view(self):\n",
    "        try:\n",
    "            self.windows_view = {'window_min_x':self.window_min_x, 'window_max_x':self.window_max_x,\n",
    "                            'window_min_y':self.window_min_y, 'window_max_y':self.window_max_y}\n",
    "            return self.windows_view\n",
    "        except:\n",
    "            print('No window view')\n",
    "\n",
    "    def get_pixel_per_metre(self):\n",
    "        return self.pixels_per_metre\n",
    "\n",
    "    def get_raw_pos(self):\n",
    "        bigx = [value[0] for value in self.raw_position['big_spotx']]\n",
    "        bigy = [value[0] for value in self.raw_position['big_spoty']]\n",
    "\n",
    "        return bigx,bigy\n",
    "    \n",
    "    \n",
    "    def filter_max_speed(self, x, y, max_speed = 3): # max speed 4m/s ()\n",
    "            tmp_x = x.copy()\n",
    "            tmp_y = y.copy()\n",
    "            for i in range(1, len(tmp_x)-1):\n",
    "                if (math.sqrt((x[i]- x[i-1])**2 + (y[i] - y[i-1])**2)) > (max_speed * self.pixels_per_metre):\n",
    "                    tmp_x[i] = 1023\n",
    "                    tmp_y[i] = 1023\n",
    "            return tmp_x, tmp_y  \n",
    "    \n",
    "    def get_position(self): \n",
    "        count_missing = 0\n",
    "        bxx, sxx = [], []\n",
    "        byy, syy = [], []\n",
    "        bigx = [value[0] for value in self.raw_position['big_spotx']]\n",
    "        bigy = [value[0] for value in self.raw_position['big_spoty']]\n",
    "        smallx = [value[0] for value in self.raw_position['little_spotx']]\n",
    "        smally = [value[0] for value in self.raw_position['little_spoty']]\n",
    "        for bx, sx in zip(bigx, smallx): # Try to clean single blocked LED x\n",
    "            if bx == 1023 and sx != 1023:\n",
    "                bx = sx\n",
    "            elif bx != 1023 and sx == 1023:\n",
    "                sx = bx\n",
    "            elif bx == 1023 and sx == 1023:\n",
    "                count_missing +=1\n",
    "                bx = np.nan\n",
    "                sx = np.nan\n",
    "    \n",
    "            bxx.append(bx)\n",
    "            sxx.append(sx)\n",
    "\n",
    "        for by, sy in zip(bigy, smally): # Try to clean single blocked LED y\n",
    "            if by == 1023 and sy != 1023:\n",
    "                by = sy\n",
    "            elif by != 1023 and sy == 1023:\n",
    "                sy = by\n",
    "            elif by == 1023 and sy == 1023:\n",
    "                by = np.nan\n",
    "                sy = np.nan\n",
    "            byy.append(by)\n",
    "            syy.append(sy)\n",
    "            \n",
    "        ### Remove coordinates with max_speed > 4ms\n",
    "        bxx, byy = self.filter_max_speed(bxx, byy)\n",
    "        sxx, syy = self.filter_max_speed(sxx, syy)\n",
    "        \n",
    "        ### Interpolate missing values\n",
    "        bxx = (pd.Series(bxx).astype(float)).interpolate('cubic')\n",
    "        sxx = (pd.Series(sxx).astype(float)).interpolate('cubic')\n",
    "        byy = (pd.Series(byy).astype(float)).interpolate('cubic')\n",
    "        syy = (pd.Series(syy).astype(float)).interpolate('cubic')\n",
    "\n",
    "        ### Average both LEDs\n",
    "        x = list((bxx + sxx)/2)\n",
    "        y = list((byy + syy)/2)\n",
    "        \n",
    "        return list(x), list(y)\n",
    "    \n",
    "    def get_speed(self):\n",
    "        print('Not implemented')\n",
    "        pass\n",
    "\n",
    "    def get_angular_pos(self):\n",
    "        print('Not implemented')\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from scipy.spatial.distance import cdist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tslearn.generators import random_walks\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "from tslearn import metrics\n",
    "\n",
    "numpy.random.seed(0)\n",
    "\n",
    "file1 = r'/mnt/d/Beths/CanCCaRet1/tmaze/s3_27022019/t2/27022019_CanCCaRet1_tmaze_3_2.eeg'\n",
    "file2 = r'/mnt/d/Beths/CanCCaRet1/tmaze/s3_27022019/t2/27022019_CanCCaRet1_tmaze_3_2.eeg3'\n",
    "\n",
    "s_x = load_lfp_Axona(file1)[0:(250*10)]\n",
    "s_y = load_lfp_Axona(file2)[0:(250*10)]\n",
    "\n",
    "s_y1 = numpy.concatenate((s_x, s_x)).reshape((-1, 1))\n",
    "s_y2 = numpy.concatenate((s_y, s_y[::-1])).reshape((-1, 1))\n",
    "sz = s_y1.shape[0]\n",
    "\n",
    "path, sim = metrics.dtw_path(s_y1, s_y2)\n",
    "\n",
    "plt.figure(1, figsize=(8, 8))\n",
    "\n",
    "# definitions for the axes\n",
    "left, bottom = 0.01, 0.1\n",
    "w_ts = h_ts = 0.2\n",
    "left_h = left + w_ts + 0.02\n",
    "width = height = 0.65\n",
    "bottom_h = bottom + height + 0.02\n",
    "\n",
    "rect_s_y = [left, bottom, w_ts, height]\n",
    "rect_gram = [left_h, bottom, width, height]\n",
    "rect_s_x = [left_h, bottom_h, width, h_ts]\n",
    "\n",
    "ax_gram = plt.axes(rect_gram)\n",
    "ax_s_x = plt.axes(rect_s_x)\n",
    "ax_s_y = plt.axes(rect_s_y)\n",
    "\n",
    "mat = cdist(s_y1, s_y2)\n",
    "\n",
    "ax_gram.imshow(mat, origin='lower')\n",
    "ax_gram.axis(\"off\")\n",
    "ax_gram.autoscale(False)\n",
    "ax_gram.plot([j for (i, j) in path], [i for (i, j) in path], \"w-\",\n",
    "             linewidth=3.)\n",
    "\n",
    "ax_s_x.plot(numpy.arange(sz), s_y2, \"b-\", linewidth=.5)\n",
    "ax_s_x.axis(\"off\")\n",
    "ax_s_x.set_xlim((0, sz - 1))\n",
    "\n",
    "ax_s_y.plot(- s_y1, numpy.arange(sz), \"b-\", linewidth=.5)\n",
    "ax_s_y.axis(\"off\")\n",
    "ax_s_y.set_ylim((0, sz - 1))\n",
    "\n",
    "# plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": true,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
